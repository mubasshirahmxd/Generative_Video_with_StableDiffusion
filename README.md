# ğŸ¥âœ¨ Generative Video with Stable Diffusion

Create AI-Generated Videos from Text Prompts Using HuggingFace Diffusers

<p align="center">
  <img src="https://img.shields.io/badge/StableDiffusion-Text2Video-blue?style=for-the-badge" />
  <img src="https://img.shields.io/badge/HuggingFace-Diffusers-yellow?style=for-the-badge" />
  <img src="https://img.shields.io/badge/Python-3.10+-blue?style=for-the-badge" />
  <img src="https://img.shields.io/badge/Colab-Ready-green?style=for-the-badge" />
  <img src="https://img.shields.io/badge/AI-Video%20Generation-purple?style=for-the-badge" />
</p>

---

# ğŸš€ Project Overview

This project demonstrates **Text-to-Video Generation** using **Stable Diffusion (Zeroscope)** via the **HuggingFace Diffusers** library.
You simply provide a text prompt and the system generates a full **AI-generated video sequence** along with individual frames.

Perfect for:

âœ”ï¸ AI content creation
âœ”ï¸ Motion design experiments
âœ”ï¸ Stable diffusion learning
âœ”ï¸ Multimodal generative AI workflows

---

# ğŸ“ Folder Structure

```
03_Generative_Video_with_StableDiffusion/
â”œâ”€ notebooks/
â”‚   â””â”€ 01_StableDiffusion_text2video.ipynb
â””â”€ output/
    â”œâ”€ output_frames/
    â”‚   â”œâ”€ frame_000.png
    â”‚   â”œâ”€ frame_001.png
    â”‚   â”œâ”€ ...
    â”‚   â””â”€ frame_035.png
    â””â”€ Sample_output.mp4
```

---

# ğŸ§  Model Used

### **ğŸ“Œ Zeroscope v2 â€” Text-to-Video Stable Diffusion Model**

HuggingFace Model:
`cerspense/zeroscope_v2_576w`

---

# ğŸ”¥ Features

- ğŸï¸ Generate videos from **any text prompt**
- ğŸ–¼ï¸ Save every generated **frame**
- ğŸ¬ Export video as `.mp4`
- âš¡ GPU optimized
- ğŸ§ª Clean notebook
- ğŸ§© Customizable

---


## âš™ï¸ Installation

### 1. Install Dependencies

Ensure you have a GPU-enabled environment (local or Google Colab).

```
pip install git+[https://github.com/huggingface/diffusers.git](https://github.com/huggingface/diffusers.git)
pip install transformers accelerate torch imageio
```

### 2. Authentication

You need a Hugging Face Access Token to download the model weights.

**Option A: Via Python Script**

```
from huggingface_hub import login
login(token="YOUR_HF_TOKEN")
```

**Option B: Environment Variable**

```
export HUGGINGFACE_TOKEN="hf_xxxxx"
```

---

ğŸ§ª Example Prompts

Try these prompts inside the notebook to generate different styles of video:

| **Style**     | **Prompt**                                                      |
| ------------------- | --------------------------------------------------------------------- |
| **Playful**   | `A cat playing with a red ball in a sunny garden`                   |
| **Cyberpunk** | `Futuristic neon-lit Tokyo street with flying cars, cinematic shot` |
| **Humorous**  | `A robot cooking biryani in a kitchen, humorous style`              |
| **Sci-Fi**    | `An astronaut walking on Mars during sunset, ultra-realistic`       |
| **Nature**    | `Beautiful waterfall flowing in slow motion, 4K nature shot`        |

---

## ğŸ§© How It Works (Pipeline Flow)

1. **Load Model:** The script initializes the `cerspense/zeroscope_v2_576w` pipeline.
2. **Inference:** You pass a text prompt.
3. **Generation:** The model produces a tensor of 36 (or more) frames.
4. **Post-Processing:** Frames are extracted and saved as PNGs.
5. **Encoding:** `imageio` stitches the frames into a standard `.mp4` file.

---

## ğŸ’¡ Future Enhancements

* [ ] Add audio generation for full video creation.
* [ ] Build a Streamlit UI for easier interaction.
* [ ] Add camera-motion effects (pan, zoom).
* [ ] Extend to long-video generation.
* [ ] Integrate with ControlNet for precise style control.

---

## â­ Author

**Mubasshir Ahmed** *Full Stack Data Science â€” Generative AI | LangChain | Diffusers*

ğŸ’› **If you found this repository helpful, please give it a â­ Star!**
